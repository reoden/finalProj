{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff855378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.3561\n",
      "Epoch 2, Average Loss: 0.2074\n",
      "Epoch 3, Average Loss: 0.1501\n",
      "Epoch 4, Average Loss: 0.1155\n",
      "Epoch 5, Average Loss: 0.0941\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 143\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions, true_labels\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# 进行预测\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# 预测新文本的函数\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_text\u001b[39m(text, model, tokenizer, device):\n",
      "Cell \u001b[1;32mIn[2], line 128\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m    125\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36mNERDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     31\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m     32\u001b[0m text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 初始化标签序列\u001b[39;00m\n\u001b[0;32m     36\u001b[0m label_seq \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "\n",
    "# 定义标签映射\n",
    "label_map = {\n",
    "    'address': 1, 'book': 2, 'company': 3, 'game': 4, 'government': 5,\n",
    "    'movie': 6, 'name': 7, 'organization': 8, 'position': 9, 'scene': 10\n",
    "}\n",
    "id2label = {v: k for k, v in label_map.items()}\n",
    "id2label[0] = 'O'  # 添加O标签表示非实体\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len=256):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                example = json.loads(line)\n",
    "                self.data.append(example)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        text = example['text']\n",
    "        labels = example['label']\n",
    "        \n",
    "        # 初始化标签序列\n",
    "        label_seq = ['O'] * len(text)\n",
    "        \n",
    "        # 填充标签序列\n",
    "        for entity_type, entities in labels.items():\n",
    "            for entity, spans in entities.items():\n",
    "                for start, end in spans:\n",
    "                    for i in range(start, end):\n",
    "                        label_seq[i] = entity_type\n",
    "        \n",
    "        # 转换为ID\n",
    "        label_ids = [label_map.get(label, 0) for label in label_seq]\n",
    "        \n",
    "        # tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 调整label以匹配tokenize后的长度\n",
    "        token_labels = [-100] * self.max_len  # -100是忽略的标签\n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                token_labels[idx] = label_ids[word_id]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(token_labels)\n",
    "        }\n",
    "\n",
    "# 初始化tokenizer和模型\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-chinese',\n",
    "    num_labels=len(label_map) + 1  # +1 for 'O' label\n",
    ")\n",
    "\n",
    "# 准备数据\n",
    "train_dataset = NERDataset('train.json', tokenizer)\n",
    "test_dataset = NERDataset('test.json', tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 训练配置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained('./ner_model')\n",
    "tokenizer.save_pretrained('./ner_model')\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(labels.numpy())\n",
    "    \n",
    "    return predictions, true_labels\n",
    "\n",
    "# 进行预测\n",
    "predictions, true_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "# 预测新文本的函数\n",
    "def predict_text(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    label_ids = predictions[0].cpu().numpy()\n",
    "    \n",
    "    results = []\n",
    "    current_entity = None\n",
    "    current_text = ''\n",
    "    \n",
    "    for token, label_id in zip(tokens, label_ids):\n",
    "        if label_id == 0 or token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            if current_entity:\n",
    "                results.append((current_text.strip(), current_entity))\n",
    "                current_entity = None\n",
    "                current_text = ''\n",
    "            continue\n",
    "        \n",
    "        if id2label[label_id] != current_entity:\n",
    "            if current_entity:\n",
    "                results.append((current_text.strip(), current_entity))\n",
    "                current_text = ''\n",
    "            current_entity = id2label[label_id]\n",
    "        \n",
    "        if token.startswith('##'):\n",
    "            current_text += token[2:]\n",
    "        else:\n",
    "            current_text += ' ' + token if current_text else token\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 示例预测\n",
    "test_text = \"加勒比海盗3：世界尽头在北京环球影城上映\"\n",
    "results = predict_text(test_text, model, tokenizer, device)\n",
    "print(\"\\n预测结果:\")\n",
    "for text, entity_type in results:\n",
    "    print(f\"文本: {text}, 类型: {entity_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5615e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原文: 我昨天去北京故宫博物院参观，然后去了长城\n",
      "提取的景点: ['北京故宫博物']\n",
      "\n",
      "原文: 去年夏天我在西湖边散步，看到了雷峰塔\n",
      "提取的景点: ['雷峰']\n",
      "\n",
      "原文: 准备明天去颐和园游玩，听说那里风景不错\n",
      "提取的景点: ['颐和']\n",
      "\n",
      "原文: 第一次到杭州什么攻略都没有做，任由同学带着到几大标志性景点转悠，到灵隐寺前还不知道它的历史文化，进去第一眼是被岩石上雕刻的栩栩如生的佛像吸引，走不久路就看到摸得光滑的“吉祥物”，应该是人人都想讨一个吉利，进去就是一线天了，然而没有指引说明，我们进去都摸不着头脑，里面基本没什么光线，而这一线天又在哪能看到？所以最好还是租个电子讲解器，也好清楚些。到灵隐寺除了拜佛烧香，剩下的就是爬山了，石梯非常多，...\n",
      "提取的景点: ['灵隐', '灵隐']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "def extract_scene(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    提取文本中的景点信息，确保完整提取词语\n",
    "    Args:\n",
    "        text: 输入文本\n",
    "        model: 加载的模型\n",
    "        tokenizer: 分词器\n",
    "        device: 设备类型（CPU/GPU）\n",
    "    Returns:\n",
    "        list: 提取出的景点列表\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 对输入文本进行编码\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 获取input_ids和attention_mask\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # 获取词片段、偏移映射和预测标签\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    offset_mapping = encoding['offset_mapping'][0].numpy()\n",
    "    label_ids = predictions[0].cpu().numpy()\n",
    "    \n",
    "    scenes = []\n",
    "    current_scene = ''\n",
    "    scene_start = None\n",
    "    \n",
    "    for idx, (token, label_id, offset) in enumerate(zip(tokens, label_ids, offset_mapping)):\n",
    "        # 跳过特殊token\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]'] or offset[0] == offset[1]:\n",
    "            continue\n",
    "        \n",
    "        # 判断是否是景点标签（scene对应的label_id为10）\n",
    "        if label_id == 10:  # scene对应的label_id\n",
    "            if scene_start is None:\n",
    "                scene_start = offset[0]\n",
    "            \n",
    "            # 检查下一个token\n",
    "            next_is_scene = False\n",
    "            if idx + 1 < len(label_ids):\n",
    "                next_is_scene = label_ids[idx + 1] == 10\n",
    "            \n",
    "            if not next_is_scene:\n",
    "                # 提取完整的景点名称\n",
    "                scene_text = text[scene_start:offset[1]]\n",
    "                if scene_text.strip():\n",
    "                    scenes.append(scene_text.strip())\n",
    "                scene_start = None\n",
    "        else:\n",
    "            scene_start = None\n",
    "    \n",
    "    return scenes\n",
    "\n",
    "# 加载保存的模型\n",
    "def load_model(model_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "    model = BertForTokenClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def process_file(input_file, output_file, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    处理整个文件中的文本并保存结果\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            text = data['text']\n",
    "            scenes = extract_scene(text, model, tokenizer, device)\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'scenes': scenes\n",
    "            })\n",
    "    \n",
    "    # 保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in results:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "def main():\n",
    "    # 加载模型\n",
    "    model_path = './ner_model'  # 替换为你的模型路径\n",
    "    model, tokenizer, device = load_model(model_path)\n",
    "    \n",
    "    # 测试文本\n",
    "    test_texts = [\n",
    "        \"我昨天去北京故宫博物院参观，然后去了长城\",\n",
    "        \"去年夏天我在西湖边散步，看到了雷峰塔\",\n",
    "        \"准备明天去颐和园游玩，听说那里风景不错\",\n",
    "        \"第一次到杭州什么攻略都没有做，任由同学带着到几大标志性景点转悠，到灵隐寺前还不知道它的历史文化，进去第一眼是被岩石上雕刻的栩栩如生的佛像吸引，走不久路就看到摸得光滑的“吉祥物”，应该是人人都想讨一个吉利，进去就是一线天了，然而没有指引说明，我们进去都摸不着头脑，里面基本没什么光线，而这一线天又在哪能看到？所以最好还是租个电子讲解器，也好清楚些。到灵隐寺除了拜佛烧香，剩下的就是爬山了，石梯非常多，...\",\n",
    "    ]\n",
    "    \n",
    "    # 测试单个文本\n",
    "    for text in test_texts:\n",
    "        scenes = extract_scene(text, model, tokenizer, device)\n",
    "        print(f\"\\n原文: {text}\")\n",
    "        print(f\"提取的景点: {scenes}\")\n",
    "    \n",
    "    # 处理文件\n",
    "    process_file(\n",
    "        input_file='test.json',\n",
    "        output_file='scenes_results.json',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2223288",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /etc/profile.d/clash.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e2915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
